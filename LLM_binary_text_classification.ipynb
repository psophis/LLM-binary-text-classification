{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psophis/LLM-binary-text-classification/blob/main/LLM_binary_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx2i8mTvAa1Y"
      },
      "source": [
        "# GPT2, BERT and T5 for binary text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_1A7VPwAkOK"
      },
      "source": [
        "Data source: https://huggingface.co/datasets/stanfordnlp/sst2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkHWQmCRoIDs"
      },
      "source": [
        "In this notebook, I will compare different LLMs on the specific NLP task of classification.\n",
        "\n",
        "**Task:** Predict text class rating (binary classification)\n",
        "\n",
        "**Outcome:**\n",
        "three trained models\n",
        "\n",
        "*   GPT2\n",
        "*   BERT\n",
        "*   T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scWef473lsDd"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjrblKRAn6q"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQQd8M6JGFpS"
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwDe65-1EgBv"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkx9YCK7A-Y3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, BertForSequenceClassification, TrainingArguments,\n",
        "    Trainer, GPT2ForSequenceClassification, DataCollatorWithPadding,\n",
        "    BertTokenizer, T5Tokenizer, T5ForConditionalGeneration, GPT2Config)\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, f1_score,\n",
        "    roc_auc_score, precision_score, recall_score, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, roc_curve, auc\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from datasets import Dataset, concatenate_datasets, load_metric\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZmgzFrQzHZf"
      },
      "outputs": [],
      "source": [
        "!pip install -q fsspec==2023.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXgqagfFFquH"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN9BQyjCArUQ"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPT4lA9qhgvw"
      },
      "source": [
        "Loading Stanford Sentiment Treebank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fksdV4jCzVl2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p glue_data/SST-2\n",
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip -o SST-2.zip -d glue_data/SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vdFYKL8_bda"
      },
      "outputs": [],
      "source": [
        "base_path = \"glue_data/SST-2/SST-2\"\n",
        "\n",
        "train_df = pd.read_csv(f\"{base_path}/train.tsv\", sep=\"\\t\")\n",
        "val_df = pd.read_csv(f\"{base_path}/dev.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(f\"{base_path}/test.tsv\", sep=\"\\t\")\n",
        "test_df = test_df.rename(columns={\"index\": \"label\"})\n",
        "\n",
        "# to HuggingFace Dataset\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "print('train: ', train_ds[0])\n",
        "print(train_ds[1])\n",
        "print('Test: ', test_ds[0])\n",
        "print('Val: ', val_ds[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLRZx6i9A2yd"
      },
      "source": [
        "## Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNaHjRsP9OTl"
      },
      "outputs": [],
      "source": [
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "label_counts = Counter(train_ds[\"label\"])\n",
        "print(\"Label distribution:\")\n",
        "for label in sorted(label_counts):\n",
        "  name = label_map[label]\n",
        "  print(f\"{name.capitalize()}: {label_counts[label]:,} samples\")\n",
        "\n",
        "labels = list(label_counts.keys())\n",
        "counts = list(label_counts.values())\n",
        "class_names = [label_map[label] for label in labels]\n",
        "\n",
        "plt.bar(class_names, counts, color=\"skyblue\")\n",
        "plt.title(\"Label distribution in training data\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVn7PDtJh18Q"
      },
      "source": [
        "I'm performing some oversampling of the negative class to ensure a balance between positive and negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrKQUM03hU_"
      },
      "outputs": [],
      "source": [
        "# split up labels\n",
        "negatives = train_ds.filter(lambda example: example[\"label\"] == 0)\n",
        "positives = train_ds.filter(lambda example: example[\"label\"] == 1)\n",
        "\n",
        "# adjust distribution\n",
        "diff = len(positives) - len(negatives)\n",
        "negatives_oversampled = negatives.shuffle(seed=42).select(range(diff))  # ZufÃ¤llige Auswahl\n",
        "\n",
        "# combine again\n",
        "train_ds = concatenate_datasets([train_ds, negatives_oversampled]).shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7XucgktGW5O"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRB8jeunKqaz"
      },
      "source": [
        "I'll evaluate each model based on these metrics:\n",
        "\n",
        "\n",
        "*   Accuracy to determine how many predictions are correct overall\n",
        "*   Balanced Accuracy to determine how well the accuracy is per class, regardless of the distribution of the classes\n",
        "*   Precision to determine how many of the positive predictions are correct\n",
        "*   Recall to determine how many positives my models find\n",
        "*   F1 score to determine how good the classification quality is, in the compromise between precision and recall\n",
        "*   ROC AUC to determine how well my models separate positive and negative classes from each other\n",
        "*   ROUGE to determine how well the predicted classes match the actual ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-67g8qrzzjI"
      },
      "outputs": [],
      "source": [
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "# metric evaluation functions\n",
        "def evaluate_model(model, dataloader, device):\n",
        "  model.eval()\n",
        "  all_labels, all_preds, all_probs = [], [], []\n",
        "  total_loss = 0                                  # accumulate loss for entire dataset\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      input_ids = batch[\"input_ids\"].to(device)             # input IDs => represent tokens from the text = Token IDs\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)   # make sure model doesn't take padding into account\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      logits = outputs.logits                     # raw scores per class (before softmax)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      probs = torch.softmax(logits, dim=1)[:, 1]  # logits to probabilities ([:, 1] --> class 1 (positive)); needed for ROC AUC\n",
        "      preds = torch.argmax(logits, dim=1)         # actual predicted class (0 or 1); needed for accuracy, precision etc.\n",
        "\n",
        "      # save all the results of the batches\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_probs.extend(probs.cpu().numpy())\n",
        "      total_loss += loss.item()                   # sum up loss per batch\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # turn num labels into text labels for rouge, to compare the string label names\n",
        "    label_map = {0: \"negative\", 1: \"positive\"}\n",
        "    text_preds = [\"positive\" if p == 1 else \"negative\" for p in all_preds]\n",
        "    text_refs  = [\"positive\" if l == 1 else \"negative\" for l in all_labels]\n",
        "    # calculate rouge\n",
        "    rouge_result = rouge_metric.compute(predictions=text_preds, references=text_refs)\n",
        "\n",
        "    # calculate all the metrics\n",
        "    metrics = {\n",
        "      \"accuracy\": accuracy_score(all_labels, all_preds),\n",
        "      \"balanced_accuracy\": balanced_accuracy_score(all_labels, all_preds),\n",
        "      \"f1\": f1_score(all_labels, all_preds),\n",
        "      \"roc_auc\": roc_auc_score(all_labels, all_probs),\n",
        "      \"precision\": precision_score(all_labels, all_preds),\n",
        "      \"recall\": recall_score(all_labels, all_preds),\n",
        "      \"confusion_matrix\": confusion_matrix(all_labels, all_preds),\n",
        "      \"val_loss\": total_loss / len(dataloader),\n",
        "      \"rouge\": rouge_result[\"rougeL\"]\n",
        "    }\n",
        "\n",
        "  return metrics, all_labels, all_preds, all_probs\n",
        "\n",
        "# plotting functions\n",
        "def plot_learning_curves(train_losses, val_losses):\n",
        "  plt.figure(figsize=(8, 4))\n",
        "  plt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
        "  plt.plot(val_losses, label=\"Validation Loss\", color=\"red\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Learning Curves\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm):\n",
        "  labels = [\"negative\", \"positive\"]\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  disp.plot(cmap=\"Blues\", values_format='d')\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "def plot_roc_curve(y_true, y_score):\n",
        "  fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.2f}\")\n",
        "  plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"ROC Curve\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHVjaHAMzn6"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6z3VdzDEMoi"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_text(input_text):\n",
        "  return tokenizer(input_text[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_ds_bert = train_ds.map(tokenize_text, batched=True)\n",
        "val_ds_bert = val_ds.map(tokenize_text, batched=True)\n",
        "test_ds_bert = test_ds.map(tokenize_text, batched=True)\n",
        "\n",
        "train_ds_bert = train_ds_bert.rename_column(\"label\", \"labels\")\n",
        "val_ds_bert = val_ds_bert.rename_column(\"label\", \"labels\")\n",
        "test_ds_bert = test_ds_bert.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# give HuggingFace the values with the 'labels'-key + format for PyTorch so I can use DataLoader\n",
        "train_ds_bert.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "val_ds_bert.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_ds_bert.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RTm9JpR-aQA"
      },
      "source": [
        "I'm freezing the layers (except the last encoder and pooler layer). Before I did this, my model showed signs of overfitting in the form of decreasing training loss and simultaneous increasing validation loss. Unfreezing only these last layers stabilized the model while speeding up training.\n",
        "\n",
        "This solution made sense since the latter transformer blocks are covering knowledge needed for specific tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULiPM99JGGDk"
      },
      "outputs": [],
      "source": [
        "# using a BERT model for binary classification\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n",
        "\n",
        "# freeze all layers except last encoder and pooler\n",
        "for name, param in model_bert.named_parameters():\n",
        "    if not name.startswith(\"bert.encoder.layer.11\") and not name.startswith(\"bert.pooler\"):\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU0NxlfWPEvI"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model_bert.to(device)\n",
        "\n",
        "if use_cuda:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "    scaler = GradScaler()\n",
        "else:\n",
        "    from contextlib import nullcontext\n",
        "    autocast = nullcontext  # use dummy context manager if no gpu is available\n",
        "    scaler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8baEonNbcz4"
      },
      "outputs": [],
      "source": [
        "#!pip install \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf15Y73sGYK8"
      },
      "outputs": [],
      "source": [
        "# DataCollatorWithPadding for correct padding within each batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "# DataLoader for efficient shuffling of training data and loading\n",
        "train_loader_bert = DataLoader(train_ds_bert, batch_size=32, shuffle=True, collate_fn=data_collator,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "val_loader_bert = DataLoader(val_ds_bert, batch_size=32, shuffle=False, collate_fn=data_collator,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "\n",
        "optimizer = Adam(filter(lambda p: p.requires_grad, model_bert.parameters()), lr=1e-5)\n",
        "\n",
        "p_itr = 1000    # print interval for my logging\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 1    # for early stopping\n",
        "counter = 0     # counter for early stopping\n",
        "\n",
        "# forward pass, calculating loss\n",
        "for epoch in range(epochs):\n",
        "  model_bert.train()\n",
        "  running_loss = 0\n",
        "  total_correct = 0\n",
        "  total_len = 0\n",
        "  itr = 1\n",
        "  temp_loss = 0\n",
        "  temp_correct = 0\n",
        "  temp_len = 0\n",
        "\n",
        "  for batch in train_loader_bert:\n",
        "      optimizer.zero_grad()\n",
        "      # load batch components to device (gpu or cpu)\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "\n",
        "      with autocast(device_type=\"cuda\"):      # activates mixed precision --> quicker on gpu\n",
        "          outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # embeddings\n",
        "          loss = outputs.loss\n",
        "          logits = outputs.logits\n",
        "\n",
        "      # backward pass, optimization\n",
        "      if scaler:\n",
        "          scaler.scale(loss).backward()\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "      else:\n",
        "          loss.backward()\n",
        "          optimizer.step() # update weights\n",
        "\n",
        "      pred = torch.argmax(logits, dim=1)    # take class with highest probability\n",
        "      correct = torch.sum(pred == labels)   # count amount of correct predictions\n",
        "      total_correct += correct.item()\n",
        "      total_len += len(labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # values for logging\n",
        "      temp_correct += correct.item()\n",
        "      temp_len += len(labels)\n",
        "      temp_loss += loss.item()\n",
        "\n",
        "      # logging\n",
        "      if itr % p_itr == 0:\n",
        "          print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(\n",
        "              epoch + 1, epochs, itr, temp_loss / p_itr, temp_correct / temp_len))\n",
        "          temp_loss = 0\n",
        "          temp_correct = 0\n",
        "          temp_len = 0\n",
        "\n",
        "      itr+=1\n",
        "\n",
        "  # calculate average\n",
        "  avg_train_loss = running_loss / len(train_loader_bert)\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "  # evaluate based on validation data\n",
        "  val_metrics_bert, y_true_bert, y_pred_bert, y_prob_bert = evaluate_model(model_bert, val_loader_bert, device)\n",
        "  val_losses.append(val_metrics_bert[\"val_loss\"])\n",
        "\n",
        "  print(f\"\\n[Epoch {epoch+1}/{epochs} Summary] --> Train Loss: {avg_train_loss:.4f}, Train Acc: {total_correct/total_len:.3f}, Val Loss: {val_metrics_bert['val_loss']:.4f}, Val Acc: {val_metrics_bert['accuracy']:.3f}\\n\")\n",
        "\n",
        "  # early stopping when validation loss stops decreasing\n",
        "  if val_metrics_bert[\"val_loss\"] < best_val_loss:\n",
        "      best_val_loss = val_metrics_bert[\"val_loss\"]\n",
        "      counter = 0\n",
        "      torch.save(model_bert.state_dict(), \"best_model_bert.pt\") # saving best model\n",
        "      print(\"â Validation loss improved, saving model\")\n",
        "  else:\n",
        "      counter += 1\n",
        "      print(f\"â ï¸ No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "      if counter >= patience:\n",
        "          print(\"â¼ï¸Early stopping triggeredâ¼ï¸\")\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z_9Wy5oUBMv"
      },
      "outputs": [],
      "source": [
        "# final plots\n",
        "plot_learning_curves(train_losses, val_losses)\n",
        "plot_confusion_matrix(val_metrics_bert[\"confusion_matrix\"])\n",
        "plot_roc_curve(y_true_bert, y_prob_bert)\n",
        "val_metrics_bert, y_true_bert, y_pred_bert, y_prob_bert = evaluate_model(model_bert, val_loader_bert, device)\n",
        "\n",
        "print(f\"\\nð Validation Metrics ð\")\n",
        "print(f\"Precision    : {val_metrics_bert['precision']:.4f}\")\n",
        "print(f\"Recall       : {val_metrics_bert['recall']:.4f}\")\n",
        "print(f\"F1-Score     : {val_metrics_bert['f1']:.4f}\")\n",
        "print(f\"Accuracy     : {val_metrics_bert['accuracy']:.4f}\")\n",
        "print(f\"Balanced Acc.: {val_metrics_bert['balanced_accuracy']:.4f}\")\n",
        "print(f\"ROC AUC      : {val_metrics_bert['roc_auc']:.4f}\")\n",
        "print(f\"ROUGE        : {val_metrics_bert['rouge']:.4f}\")\n",
        "print(f\"Validation Loss: {val_metrics_bert['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdOaHiw8JMR8"
      },
      "source": [
        "## GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeXlvPcCk-QG"
      },
      "outputs": [],
      "source": [
        "# tokenization\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "def tokenize_text_gpt2(input_text):\n",
        "  return gpt2_tokenizer(input_text[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_ds_gpt2 = train_ds.map(tokenize_text_gpt2, batched=True)\n",
        "val_ds_gpt2 = val_ds.map(tokenize_text_gpt2, batched=True)\n",
        "test_ds_gpt2 = test_ds.map(tokenize_text_gpt2, batched=True)\n",
        "\n",
        "train_ds_gpt2.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_ds_gpt2.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_ds_gpt2.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_kpm5vomogA"
      },
      "outputs": [],
      "source": [
        "# using GPT2 model for classification tasks\n",
        "config = GPT2Config.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2\n",
        ")\n",
        "model_gpt2 = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
        "model_gpt2.config.pad_token_id = model_gpt2.config.eos_token_id\n",
        "\n",
        "# freeze lower layers\n",
        "for i, block in enumerate(model_gpt2.transformer.h):\n",
        "    if i < 8:\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KhjUaVZnZ-y"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "data_collator = DataCollatorWithPadding(tokenizer=gpt2_tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "train_loader_gpt2 = DataLoader(train_ds_gpt2, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "val_loader_gpt2 = DataLoader(val_ds_gpt2, batch_size=16, shuffle=False, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8khQHHaynujE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_gpt2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObHMuD6q1aWT"
      },
      "outputs": [],
      "source": [
        "#!pip install \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIP1HFsZp09-"
      },
      "outputs": [],
      "source": [
        "optimizer_gpt2 = Adam(model_gpt2.parameters(), lr=1e-5)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 1\n",
        "counter = 0\n",
        "epochs = 3\n",
        "p_itr = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_gpt2.train()\n",
        "    running_loss = 0\n",
        "    total_correct = 0\n",
        "    total_len = 0\n",
        "    temp_loss = 0\n",
        "    temp_correct = 0\n",
        "    temp_len = 0\n",
        "    itr = 1\n",
        "\n",
        "    for batch in train_loader_gpt2:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer_gpt2.zero_grad()      # reset gradients to zero\n",
        "        outputs = model_gpt2(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_gpt2.step()\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        correct = torch.sum(pred == labels)\n",
        "        running_loss += loss.item()\n",
        "        total_correct += correct.item()\n",
        "        total_len += len(labels)\n",
        "\n",
        "        temp_correct += correct.item()\n",
        "        temp_len += len(labels)\n",
        "        temp_loss += loss.item()\n",
        "\n",
        "        if itr % p_itr == 0:\n",
        "            print(f\"[Epoch {epoch+1}/{epochs}] Iteration {itr} -> Train Loss: {temp_loss/p_itr:.4f}, Accuracy: {temp_correct/temp_len:.3f}\")\n",
        "            temp_loss = 0\n",
        "            temp_correct = 0\n",
        "            temp_len = 0\n",
        "\n",
        "        itr += 1\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader_gpt2)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluation\n",
        "    val_metrics, y_true, y_pred, y_prob = evaluate_model(model_gpt2, val_loader_gpt2, device)\n",
        "    val_losses.append(val_metrics[\"val_loss\"])\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch+1}/{epochs} Summary] --> Train Loss: {avg_train_loss:.4f}, Train Acc: {total_correct/total_len:.3f}, Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['accuracy']:.3f}\")\n",
        "    print(f\"F1: {val_metrics['f1']:.4f} | Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f} | ROC AUC: {val_metrics['roc_auc']:.4f} | ROUGE: {val_metrics['rouge']:.4f}\\n\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_metrics[\"val_loss\"] < best_val_loss:\n",
        "        best_val_loss = val_metrics[\"val_loss\"]\n",
        "        counter = 0\n",
        "        torch.save(model_gpt2.state_dict(), \"best_model_gpt2.pt\")\n",
        "        print(\"â Validation loss improved, saving model\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"â ï¸ No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"â¼ï¸ Early stopping triggered â¼ï¸\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ4Vnh3osNbR"
      },
      "outputs": [],
      "source": [
        "# final plots\n",
        "plot_learning_curves(train_losses, val_losses)\n",
        "plot_confusion_matrix(val_metrics[\"confusion_matrix\"])\n",
        "plot_roc_curve(y_true, y_prob)\n",
        "val_metrics_gpt2, y_true_gpt2, y_pred_gpt2, y_prob_gpt2 = evaluate_model(model_gpt2, val_loader_gpt2, device)\n",
        "\n",
        "print(f\"\\nð Validation Metrics\")\n",
        "print(f\"Precision    : {val_metrics_gpt2['precision']:.4f}\")\n",
        "print(f\"Recall       : {val_metrics_gpt2['recall']:.4f}\")\n",
        "print(f\"F1-Score     : {val_metrics_gpt2['f1']:.4f}\")\n",
        "print(f\"Accuracy     : {val_metrics_gpt2['accuracy']:.4f}\")\n",
        "print(f\"Balanced Acc.: {val_metrics_gpt2['balanced_accuracy']:.4f}\")\n",
        "print(f\"ROC AUC      : {val_metrics_gpt2['roc_auc']:.4f}\")\n",
        "print(f\"ROUGE        : {val_metrics_gpt2['rouge']:.4f}\")\n",
        "print(f\"Validation Loss: {val_metrics_gpt2['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGTTRyyepwoP"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Ds4gGVIpu-i"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "def tokenize_text(example):\n",
        "  input_texts = [\"Sentiment: \" + s for s in example[\"sentence\"]]\n",
        "  target_texts = [\"positive\" if l == 1 else \"negative\" for l in example[\"label\"]]   # labels as strings\n",
        "\n",
        "  # input tokenization\n",
        "  model_inputs = tokenizer_t5(input_texts, text_target=target_texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "  # target tokenization\n",
        "  with tokenizer_t5.as_target_tokenizer():\n",
        "    labels = tokenizer_t5(target_texts, padding=\"max_length\", truncation=True, max_length=3)\n",
        "\n",
        "  label_ids = labels[\"input_ids\"]\n",
        "  label_ids = [\n",
        "        [(token if token != tokenizer_t5.pad_token_id else -100) for token in label_seq]\n",
        "        for label_seq in label_ids\n",
        "  ]\n",
        "  model_inputs[\"labels\"] = label_ids\n",
        "  return model_inputs\n",
        "\n",
        "train_ds_t5 = train_ds.map(tokenize_text, batched=True)\n",
        "val_ds_t5 = val_ds.map(tokenize_text, batched=True)\n",
        "test_ds_t5 = test_ds.map(tokenize_text, batched=True)\n",
        "\n",
        "train_ds_t5.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "val_ds_t5.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_ds_t5.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DXUncydp6R_"
      },
      "outputs": [],
      "source": [
        "# preparing the model\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "# freeze everything (encoder, decoder, position embeddings)\n",
        "for param in model_t5.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze last layer of language model\n",
        "for param in model_t5.lm_head.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-WHAjZFqJXH"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_t5, return_tensors=\"pt\")\n",
        "\n",
        "train_loader_t5 = DataLoader(train_ds_t5, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "val_loader_t5 = DataLoader(val_ds_t5, batch_size=16, shuffle=False, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxi8158mqKy-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t5.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz69dNokqN_s"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "optimizer_t5 = Adam(model_t5.parameters(), lr=1e-5)\n",
        "epochs = 3\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 1\n",
        "counter = 0\n",
        "p_itr = 1000\n",
        "total_preds = []\n",
        "total_labels = []\n",
        "temp_preds = []\n",
        "temp_labels = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_t5.train()\n",
        "    running_loss = 0\n",
        "    temp_loss = 0\n",
        "    itr = 1\n",
        "    temp_preds = []\n",
        "    temp_labels = []\n",
        "\n",
        "    for batch in train_loader_t5:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer_t5.zero_grad()\n",
        "        outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer_t5.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        temp_loss += loss.item()\n",
        "\n",
        "        # prediction\n",
        "        generated_ids = model_t5.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=3)\n",
        "        preds = tokenizer_t5.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        labels_clean = labels.clone()\n",
        "        labels_clean[labels_clean == -100] = tokenizer_t5.pad_token_id\n",
        "        targets = tokenizer_t5.batch_decode(labels_clean, skip_special_tokens=True)\n",
        "\n",
        "        temp_preds.extend(preds)\n",
        "        temp_labels.extend(targets)\n",
        "\n",
        "        if itr % p_itr == 0:\n",
        "            acc = accuracy_score(temp_labels, temp_preds)\n",
        "            print(f\"[Epoch {epoch+1}/{epochs}] Iteration {itr} -> Train Loss: {temp_loss/p_itr:.4f}, Accuracy: {acc:.3f}\")\n",
        "            temp_loss = 0\n",
        "            temp_preds = []\n",
        "            temp_labels = []\n",
        "        itr += 1\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader_t5)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # evaluation + early stopping\n",
        "    model_t5.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader_t5:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader_t5)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch+1}/{epochs} Summary] --> Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\\n\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model_t5.state_dict(), \"best_model_t5.pt\")\n",
        "        print(\"â Validation loss improved, saving model\\n\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"â ï¸ No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"â¼ï¸ Early stopping triggered â¼ï¸\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5N_3sYHNPaz"
      },
      "outputs": [],
      "source": [
        "# load rouge metric module\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "# eval prep\n",
        "model_t5.eval()                                 # put model in evaluation mode\n",
        "all_labels, all_preds = [], []                  # for numeric labels\n",
        "decoded_preds, decoded_labels = [], []          # for text output for rouge calculation\n",
        "\n",
        "# normalize text\n",
        "def normalize_label(text):\n",
        "    return text.strip().lower().replace(\".\", \"\").replace(\"!\", \"\").replace(\" \", \"\")\n",
        "\n",
        "# mapping text label to num label\n",
        "str2label = {\"negative\": 0, \"positive\": 1}\n",
        "\n",
        "# evaluation loop\n",
        "with torch.no_grad():                                         # no gradient computed, to save time and memory\n",
        "    for batch in val_loader_t5:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # generate predictions\n",
        "        generated_ids = model_t5.generate(                    # generate output text, e.g. \"positive\"\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=3\n",
        "        )\n",
        "\n",
        "        # decoding\n",
        "        pred_texts = tokenizer_t5.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        labels_clean = labels.clone()\n",
        "        labels_clean[labels_clean == -100] = tokenizer_t5.pad_token_id\n",
        "        label_texts = tokenizer_t5.batch_decode(labels_clean, skip_special_tokens=True)\n",
        "\n",
        "        # save textual values for rouge\n",
        "        decoded_preds.extend(pred_texts)\n",
        "        decoded_labels.extend(label_texts)\n",
        "\n",
        "        # mapping to num labels for classification\n",
        "        for ref, pred in zip(label_texts, pred_texts):\n",
        "            ref_norm = normalize_label(ref)\n",
        "            pred_norm = normalize_label(pred)\n",
        "            if ref_norm in str2label and pred_norm in str2label:        # \"negative\": 0, \"positive\": 1\n",
        "                all_labels.append(str2label[ref_norm])\n",
        "                all_preds.append(str2label[pred_norm])\n",
        "\n",
        "# if the model didn't produce any valid predictions or labels set metrics to zero\n",
        "if len(all_labels) == 0 or len(all_preds) == 0:\n",
        "    print(\"â ï¸ No valid predictions or labels, all metrics skipped\")\n",
        "    val_metrics_t5 = {\n",
        "        \"accuracy\": 0.0,\n",
        "        \"balanced_accuracy\": 0.0,\n",
        "        \"f1\": 0.0,\n",
        "        \"precision\": 0.0,\n",
        "        \"recall\": 0.0,\n",
        "        \"confusion_matrix\": [[0, 0], [0, 0]],\n",
        "        \"rouge\": 0.0,\n",
        "    }\n",
        "else:\n",
        "    # calc rouge: rate text similarity between predicted and actual\n",
        "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # calc metrics\n",
        "    val_metrics_t5 = {\n",
        "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(all_labels, all_preds),\n",
        "        \"f1\": f1_score(all_labels, all_preds, zero_division=0),\n",
        "        \"precision\": precision_score(all_labels, all_preds, zero_division=0),\n",
        "        \"recall\": recall_score(all_labels, all_preds, zero_division=0),\n",
        "        \"confusion_matrix\": confusion_matrix(all_labels, all_preds),\n",
        "        \"rouge\": rouge_result[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "    # plots\n",
        "    plot_learning_curves(train_losses, val_losses)\n",
        "    plot_confusion_matrix(val_metrics_t5[\"confusion_matrix\"])\n",
        "\n",
        "# output\n",
        "print(f\"\\nð Validation Metrics\")\n",
        "for k, v in val_metrics_t5.items():\n",
        "    if k.startswith(\"rouge\") or k.endswith(\"accuracy\") or k in [\"f1\", \"precision\", \"recall\"]:\n",
        "        print(f\"{k:<18}: {v:.4f}\")\n",
        "\n",
        "# debug with examples\n",
        "print(\"\\nsome examples\")\n",
        "for i in range(min(5, len(decoded_preds))):\n",
        "    print(f\"actual: '{decoded_labels[i]}' --> pred: '{decoded_preds[i]}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkN1W6WZlXzc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The training of BERT was quick and brought good results, while GPT-2 shows signs of overfitting even after unfreezing only the top layers. The T5 model shows a lot of promise, differentiating well between the classes.\n",
        "At this point, I'd continue tweaking BERT and T5 to optimize the results, but wouldn't spend more ressources on GPT-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuRxGF_MI9zp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Sources:**\n",
        "\n",
        "I used GitHub repos and Colab notebooks, as inspiration and guidance for how to accomplish this task\n",
        "BERT: https://github.com/zzaebok/PytorchBertExample/blob/master/BertForSequenceClassification_%EC%98%88%EC%A0%9C.ipynb\n",
        "\n",
        "GPT2: https://colab.research.google.com/drive/1dMTdO5vxdVX0NA2Qe7AV9WGEy8ZH67Xn?usp=sharing#scrollTo=7dc4a339\n",
        "\n",
        "T5: https://www.kaggle.com/code/prithvijaunjale/t5-multi-label-classification/notebook\n",
        "\n",
        "I used the help of ChatGPT to debug when I had errors I didn't manage to fix on my own, especially for the T5 model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iN9BQyjCArUQ",
        "LTHVjaHAMzn6"
      ],
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyP7eIwUCIdD8Vi4OES1LbsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}